kubernetes的ceph RBD volume(1)：使用Ceph RBD作为后端Volume<br/><br/><div class="article_content tracking-ad" data-dsm="post" data-mod="popu_307" id="article_content">

<p><br/>
</p>
<p><span style="background-color:rgb(255,255,255)"></span><br/>
</p>
<p>Kubernetes使用Ceph RBD作为后端Volume。 Kubernetes的官方源码 的examples/volumes/rbd目录下，就有一个使用cephrbd作为kubernetes pod volume的例子，我们可以参考<br/>
<br/>
<br/>
</p>
<p>1. 当ceph集群安装完成以后，我们就要创建相应的rbd块用于kubernetes存储。创建块设备之前，需要先创建存储池</p>
<p><br/>
ceph osd pool create kube 256 256    #后面两个256分别为pg-num和pgp-num</p>
<p>ceph osd pool ls detail</p>
<p>pool 4 'kube' replicated size 3 min_size 2 crush_ruleset 0 object_hash rjenkins pg_num 256 pgp_num 256 last_change 2410 flags hashpspool stripe_width 0<br/>
    removed_snaps [1~3]</p>
<p><br/>
</p>
<p>2. 在kube存储池创建一个映像文件，就叫vol50，该映像文件的大小为50GB：<br/>
<br/>
rbd create kube/vol50 --size 50000</p>
<p> rbd -p kube info vol50<br/>
rbd image 'vol50':<br/>
    size 51200 MB in 12800 objects<br/>
    order 22 (4096 kB objects)<br/>
    block_name_prefix: rb.0.754829.238e1f29<br/>
    format: 1<br/>
</p>
<p>3. 创建用户client.kube 用admin用户<br/>
</p>
<p>ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read class-write object_prefix rbd_children, allow rwx pool=kube' -o ceph.client.kube.keyring</p>
<p>通常我们在ceph install时在ceph.conf中使用默认的安全验证协议 cephx – The Ceph authentication protocol 了。<br/>
</p>
<p>4.  生成secret<br/>
</p>
<p>得到key(base64)<br/>
</p>
<p>grep key /etc/ceph/ceph.client.kube.keyring |awk '{printf "%s", $NF}'|base64</p>
<p>QVFCK0l4RlpqK0xDTkJBQTRKYVBPTWx6WkFIVVhLK2ozM2lQdUE9PQo=</p>
<p>写secret.yaml：<br/>
</p>
<p>[root@testnew rbd]# cat ceph-secret.yaml<br/>
apiVersion: v1<br/>
kind: Secret<br/>
metadata:<br/>
  name: ceph-secret<br/>
type: "kubernetes.io/rbd"<br/>
data:<br/>
  key: QVFCK0l4RlpqK0xDTkJBQTRKYVBPTWx6WkFIVVhLK2ozM2lQdUE9PQo=</p>
<p> copy /etc/ceph/ceph.client.kube.keyring和ceph.conf到kubernetes的所有节点</p>
<p>kubectl create -f ceph-secret.yaml</p>
<p>[root@testnew ~]# kubectl get secret<br/>
NAME          TYPE                DATA      AGE<br/>
ceph-secret   kubernetes.io/rbd   1         13d<br/>
</p>
<p>5. 格式化一个空image<br/>
</p>
<p>格式化一个空image那样对其进行格式化了，这里格成ext4文件系统（格式化这一步可以不需要）</p>
<p>rbd map kube/vol50</p>
<p>rbd info kube/vol50<br/>
</p>
<p>mkfs.ext4 /dev/rbd0</p>
<p>rbd unmap /dev/rbd0<br/>
</p>
<p>6. 创建pod with RBD</p>
<p>[root@testnew kube]# cat frontend-rbd-controller.yaml<br/>
apiVersion: v1<br/>
kind: ReplicationController<br/>
metadata:<br/>
  name: frontendrbd1<br/>
  labels:<br/>
    name: frontendrbd1<br/>
spec:<br/>
  replicas: 1<br/>
  selector:<br/>
    name: frontendrbd1<br/>
  template:<br/>
    metadata:<br/>
      labels:<br/>
        name: frontendrbd1<br/>
    spec:<br/>
      containers:<br/>
      - name: frontendrbd1<br/>
        image: kubeguide/guestbook-php-frontend<br/>
        env:<br/>
        - name: GET_HOSTS_FROM<br/>
          value: env<br/>
        ports:<br/>
        - containerPort: 80<br/>
        volumeMounts:<br/>
        - mountPath: /mnt/rbd<br/>
          name: rbdpb<br/>
      volumes:<br/>
      - name: rbdpb<br/>
        rbd:<br/>
          monitors:<br/>
          - 10.0.200.11:6789<br/>
          - 10.0.200.13:6789<br/>
          - 10.0.200.14:6789<br/>
          pool: kube<br/>
          image: vol50<br/>
          user: kube<br/>
          secretRef:<br/>
              name: ceph-secret<br/>
          fsType: ext4<br/>
          readOnly: false</p>
<p>kubectl create -f frontend-rbd-controller.yaml</p>
<p>[root@testnew ~]# kubectl get rc<br/>
NAME            DESIRED   CURRENT   READY     AGE<br/>
frontendrbd1    1         1         1         13d<br/>
<br/>
[root@testnew ~]# kubectl get pods<br/>
NAME                  READY     STATUS    RESTARTS   AGE<br/>
frontendrbd1-h9z78    1/1       Running   1          13d</p>
<p>7. 验证volume在container里。</p>
<p>[root@testnew ~]# kubectl exec frontendrbd1-h9z78 -it bash<br/>
root@frontendrbd1-h9z78:/var/www/html# df -k<br/>
Filesystem                                                                                       1K-blocks     Used Available Use% Mounted on<br/>
/dev/mapper/docker-253:1-530097-861967a5b3b1a5f40b4880db1921a52af2656a10bf5ce9d1727c40845a4aa9c2  10474496   623084   9851412   6% /<br/>
tmpfs                                                                                              4087712        0   4087712   0% /dev<br/>
tmpfs                                                                                              4087712        0   4087712   0% /sys/fs/cgroup<br/>
<span style="color:#FF0000">/dev/rbd0                                                                                         51474912 16840936  31996152  35% /mnt/rbd</span><br/>
/dev/vda1                                                                                         19593296 15144980   3577460  81% /etc/hosts<br/>
shm                                                                                                  65536        0     65536   0% /dev/shm<br/>
<br/>
<br/>
</p>
<p><br/>
</p>
<p><br/>
</p>
<p><br/>
</p>
<p><br/>
<br/>
</p>
   
</div>
